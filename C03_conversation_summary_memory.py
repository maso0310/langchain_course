from langchain_community.llms import Ollama
from langchain.chains import ConversationChain
from langchain.memory import ConversationSummaryMemory
from langchain_core.prompts import PromptTemplate

# ğŸ¦™ æŒ‡å®šä½¿ç”¨çš„æ¨¡å‹
llm = Ollama(model="gemma3")

# ğŸ§  å»ºç«‹æ‘˜è¦å¼è¨˜æ†¶é«”ï¼ˆæœƒè‡ªå‹•ç”¢ç”Ÿç¸½çµï¼‰
memory = ConversationSummaryMemory(llm=llm, return_messages=True)

# âœ¨ è‡ªè¨‚æç¤ºæ¨¡æ¿ï¼šä¸€æ¨£è¦æ±‚ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
custom_prompt = PromptTemplate.from_template("""è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹çš„å•é¡Œã€‚
{history}
äººé¡ï¼š{input}
AIï¼š""")

# ğŸ’¬ å»ºç«‹å°è©±éˆï¼Œä¸²æ¥è¨˜æ†¶é«”èˆ‡æç¤ºæ¨¡æ¿
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=custom_prompt,
    verbose=True
)

# ğŸ—£ï¸ æ¨¡æ“¬å°è©±ï¼šé€²è¡Œä¸‰è¼ªå•ç­”
conversation.predict(input="ä½ å¥½ï¼Œå¯ä»¥å¹«æˆ‘è¦åŠƒä¸€ä¸‹å°å—ä¸‰æ—¥æ—…éŠè¡Œç¨‹å—ï¼Ÿ")
conversation.predict(input="æˆ‘æƒ³è¦ç¬¬äºŒå¤©å»é—œå­å¶ºæ³¡æº«æ³‰ï¼Œé€™æ¨£å®‰æ’åˆç†å—ï¼Ÿ")
response = conversation.predict(input="å¹«æˆ‘ç°¡å–®ç¸½çµä¸€ä¸‹ç›®å‰çš„è¡Œç¨‹è¦åŠƒ")

print("ç¸½çµå›æ‡‰ï¼š", response)
