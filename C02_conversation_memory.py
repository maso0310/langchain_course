from langchain_community.llms import Ollama
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate

# ğŸ¦™ ä½¿ç”¨ gemma3 æ¨¡å‹
llm = Ollama(model="gemma3")

# ğŸ§  å»ºç«‹è¨˜æ†¶é«”ï¼Œè¨˜éŒ„ä¸Šä¸‹æ–‡å°è©±
memory = ConversationBufferMemory(return_messages=True)

# âœ¨ è‡ªè¨‚æç¤ºæ¨¡æ¿ï¼šè¦æ±‚ç”¨ç¹é«”ä¸­æ–‡å›ç­”
custom_prompt = PromptTemplate.from_template("""
è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ä»¥ä¸‹çš„å•é¡Œã€‚
{history} 
äººé¡ï¼š{input} 
AIï¼š""")

# ğŸ’¬ å»ºç«‹å°è©±éˆï¼ŒæŒ‡å®šä½¿ç”¨è‡ªè¨‚æç¤ºæ¨¡æ¿
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    prompt=custom_prompt,
    verbose=True
)

# ğŸ—£ï¸ åŸ·è¡Œå…©è¼ªå•ç­”
response1 = conversation.predict(input="è«‹å•ä½ æ˜¯èª°ï¼Ÿ")
print("ç¬¬ä¸€è¼ªå›æ‡‰ï¼š", response1)

response2 = conversation.predict(input="ä½ å‰›å‰›èªªä½ æ˜¯ AIï¼Œé‚£ä½ èƒ½åšä»€éº¼ï¼Ÿ")
print("ç¬¬äºŒè¼ªå›æ‡‰ï¼š", response2)
