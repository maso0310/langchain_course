from langchain_core.runnables import ConfigurableField
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_ollama import OllamaLLM

# å®šç¾©ä¸‰ç¨® Prompt æ¨¡æ¿
prompt = PromptTemplate.from_template("ç”¨ç¹é«”ä¸­æ–‡åƒæœ‹å‹èŠå¤©å›æ‡‰æˆ‘é€™æ®µè©±ï¼š{topic}").configurable_alternatives(
    ConfigurableField(id="prompt", name="Prompt", description="æè©çµ„åˆ"),
    default_key="friendly",
    formal=PromptTemplate.from_template("ç”¨ç¹é«”ä¸­æ–‡æ­£å¼åœ°å›æ‡‰æˆ‘é€™æ®µè©±ï¼š{topic}"),
    wiseman=ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€å€‹ç¿æ™ºçš„è€äºº"),
        ("human", "ç”¨èªªæ•…äº‹çš„æ–¹å¼ï¼Œç”¨ç¹é«”ä¸­æ–‡å›æ‡‰æˆ‘çš„é€™æ®µè©±ï¼š{topic}")
    ]),
)

# å®šç¾©ä¸‰ç¨® Ollama æ¨¡å‹ï¼ˆgemma3ã€llama3 å’Œ mistralï¼‰
llm = OllamaLLM(model="gemma3").configurable_alternatives(
    ConfigurableField(id="llm", name="LLM", description="èªè¨€æ¨¡å‹"),
    default_key="gemma3",
    llama3=OllamaLLM(model="llama3"),
    mistral=OllamaLLM(model="mistral")
)

# ä¸²æ¥æ•´å€‹ chainï¼šPrompt â†’ LLM â†’ Parser
chain = prompt | llm | StrOutputParser()

# æ¸¬è©¦é è¨­çµ„åˆï¼ˆfriendly + gemma3ï¼‰
print("ğŸŸ¢ é è¨­çµ„åˆï¼ˆfriendly + gemma3ï¼‰ï¼š", chain.invoke({"topic": "è«‹å•å¦‚ä½•è¦åŠƒäººç”Ÿæ–¹å‘ï¼Ÿ"}))

# æ¸¬è©¦ï¼šæ”¹æˆ formal + llama3
chain_custom = chain.with_config(configurable={
    "prompt": "formal",
    "llm": "llama3"
})
print("\nğŸ”µ åˆ‡æ›çµ„åˆï¼ˆ formal + llama3 ï¼‰ï¼š", chain_custom.invoke({"topic": "è«‹å•å¦‚ä½•è¦åŠƒäººç”Ÿæ–¹å‘ï¼Ÿ"}))

# æ¸¬è©¦ï¼šæ”¹æˆ wiseman + mistral
chain_custom = chain.with_config(configurable={
    "prompt": "wiseman",
    "llm": "mistral"
})
print("\nğŸ”µ åˆ‡æ›çµ„åˆï¼ˆ wiseman + mistral ï¼‰ï¼š", chain_custom.invoke({"topic": "è«‹å•å¦‚ä½•è¦åŠƒäººç”Ÿæ–¹å‘ï¼Ÿ"}))
