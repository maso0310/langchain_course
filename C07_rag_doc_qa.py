from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import Ollama

# 1ï¸âƒ£ è®€å–æ–‡ä»¶
loader = TextLoader("reference.txt", encoding="utf-8")
documents = loader.load()

# 2ï¸âƒ£ æ–‡ä»¶åˆ‡å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,
    chunk_overlap=100,
    add_start_index=True
)
docs = text_splitter.split_documents(documents)

# 3ï¸âƒ£ å»ºç«‹å‘é‡è³‡æ–™åº«ï¼ˆæ”¹æˆ Chromaï¼‰
embedding = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = Chroma.from_documents(docs, embedding)

# 4ï¸âƒ£ å•Ÿå‹• QA ç³»çµ±
llm = Ollama(model="gemma3")
retriever = vectorstore.as_retriever(
    search_type="mmr",  # å¤šæ¨£æ€§å°å‘
    search_kwargs={"k": 3}  # å–å›3å€‹ç›¸é—œæ®µè½ä»¥ä¾›çµ„åˆ
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# 5ï¸âƒ£ ç™¼å•
queries = [
    "LangChain æ˜¯ä»€éº¼ï¼Ÿ",
    "LangChain æ”¯æ´å“ªäº›å‘é‡è³‡æ–™åº«ï¼Ÿ",
    "å¦‚ä½•ä½¿ç”¨ LangChain å»ºç«‹æ–‡ä»¶å•ç­”ç³»çµ±ï¼Ÿ",
    "LangChain çš„å…­å¤§æ¨¡çµ„åŒ…æ‹¬å“ªäº›ï¼Ÿ",
    "å“ªäº›æ‡‰ç”¨å¯ä»¥ä½¿ç”¨ LangChain ä¾†å¯¦ä½œï¼Ÿ"
]
for query in queries:
    prompt = f"è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š{query}"
    result = qa.invoke({"query": prompt})
    print(f"\nğŸŸ¦ å•é¡Œï¼š{query}")
    print("âœ… å›ç­”ï¼š", result["result"])
