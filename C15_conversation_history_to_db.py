from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chains import ConversationChain
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
import sqlite3
from langchain.chains.summarize import load_summarize_chain
from langchain_core.documents import Document
from langchain_core.runnables import RunnableLambda
from operator import itemgetter

# âœ… åˆå§‹åŒ– LLM æ¨¡å‹
llm = OllamaLLM(model="gemma3")

# âœ… å»ºç«‹æ‘˜è¦ç”¨ Prompt
summary_prompt = ChatPromptTemplate.from_template(
    "é€™æ˜¯ç›®å‰çš„å°è©±æ‘˜è¦ï¼š\n\n{summary}\n\né€™æ˜¯æ–°çš„å°è©±å…§å®¹ï¼š\n\n{new_lines}\n\nè«‹ç”¨ä¸­æ–‡æ›´æ–°å°è©±æ‘˜è¦ï¼š"
)

# âœ… å»ºç«‹å°è©±è¨˜æ†¶éˆ
def create_chain(assistant):
    chat_prompt = ChatPromptTemplate.from_messages([
        ("system", f"ä½ æ˜¯å€‹{assistant}ï¼Œè«‹æ ¹æ“šå°è©±ä½œå›æ‡‰"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}")
    ])

    history_db = SQLChatMessageHistory(
        session_id=assistant,
        connection_string='sqlite:///historyMemory.db',
        table_name="chat_memory"
    )

    memory = ConversationSummaryBufferMemory(
        llm=llm,
        prompt=summary_prompt,
        return_messages=True,
        chat_memory=history_db,
        max_token_limit=1000  # å¯ä»¥ä¾éœ€è¦èª¿æ•´æ‘˜è¦ä¸Šé™
    )

    # âœ… æ”¹ç”¨ load_memory_variables ä¾†æª¢æŸ¥æ˜¯å¦å·²æœ‰æ‘˜è¦
    current_summary = memory.load_memory_variables({}).get("history", "")
    if not current_summary:
        messages = history_db.messages
        if messages:
            # âœ… ç”¢ç”Ÿæ‘˜è¦ï¼ˆåªæœ‰ç¬¬ä¸€æ¬¡ç”¨ï¼‰
            summary_text = summarize_existing_history(messages)
            memory.moving_summary_buffer = summary_text
            print(f"ğŸ§  å¾æ­·å²è³‡æ–™ä¸­ç”¢ç”Ÿæ‘˜è¦å¦‚ä¸‹ï¼š\n{summary_text}\n")
        else:
            print("ğŸ§  å°šç„¡è¨˜æ†¶ç´€éŒ„ï¼Œé€™æ˜¯ä½ å€‘çš„ç¬¬ä¸€æ¬¡å°è©±ã€‚\n")

    return chat_prompt, memory

def list_assistants():
    conn = sqlite3.connect("historyMemory.db")
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT DISTINCT session_id FROM chat_memory")
        assistants = [row[0] for row in cursor.fetchall()]
    except sqlite3.OperationalError:
        assistants = []  # è¡¨æ ¼å°šæœªå»ºç«‹
    conn.close()
    return assistants

# â¬‡ï¸ è‡ªè¨‚ã€Œç¹é«”ä¸­æ–‡ã€æ‘˜è¦æç¤ºè©
custom_stuff_prompt = PromptTemplate.from_template(
    "ä½ æ˜¯ä¸€å€‹å–„æ–¼ä¸­æ–‡ç¸½çµçš„åŠ©ç†ã€‚è«‹é–±è®€ä»¥ä¸‹å°è©±å…§å®¹ï¼Œä¸¦ç”¨ç¹é«”ä¸­æ–‡ç°¡è¦ç¸½çµå‡ºå°è©±çš„é‡é»ï¼š\n\n{text}\n\nç¹é«”ä¸­æ–‡æ‘˜è¦ï¼š"
)

def summarize_existing_history(messages):
    if not messages:
        return ""
    
    text = "\n\n".join([f"{'ä½¿ç”¨è€…' if m.type == 'human' else 'åŠ©ç†'}ï¼š{m.content}" for m in messages])
    docs = [Document(page_content=text)]

    # ä½¿ç”¨è‡ªè¨‚ prompt å»ºç«‹ summarize chain
    summarize_chain = load_summarize_chain(
        llm=llm,
        chain_type="stuff",
        prompt=custom_stuff_prompt
    )
    return summarize_chain.run(docs)

# ä¸»ç¨‹å¼æµç¨‹
if __name__ == "__main__":
    existing = list_assistants()
    if existing:
        print("ğŸ“‹ ç›®å‰å·²æœ‰è¨˜æ†¶çš„åŠ©ç†æœ‰ï¼š")
        for name in existing:
            print(f" - {name}")
    else:
        print("ğŸ“‹ ç›®å‰é‚„æ²’æœ‰ä»»ä½•è¨˜æ†¶ä¸­çš„åŠ©ç†ã€‚")

    sys_msg = input('è«‹è¨­å®šåŠ©ç†åç¨±ï¼š')
    if not sys_msg.strip():
        sys_msg = 'å°åŠ©ç†'

    chat_prompt, memory = create_chain(sys_msg)

    # âœ… é¡¯ç¤ºè©²åŠ©ç†çš„è¨˜æ†¶æ‘˜è¦
    print(f"\nğŸ” æ­¡è¿å›ä¾†ï¼Œ{sys_msg}ï¼")
    summary_data = memory.load_memory_variables({}).get("history", [])
    if summary_data and isinstance(summary_data, list):
        summary_text = summary_data[0].content
        print(f"ğŸ§  ä¸Šæ¬¡çš„æ‘˜è¦è¨˜æ†¶å¦‚ä¸‹ï¼š\n{summary_text}\n")
    else:
        print("ğŸ§  å°šç„¡è¨˜æ†¶ç´€éŒ„ï¼Œé€™æ˜¯ä½ å€‘çš„ç¬¬ä¸€æ¬¡å°è©±ã€‚\n")

    # âœ… å»ºç«‹æœƒè©±éˆï¼ˆé€é LCEL çµ„åˆï¼‰
    chain = ConversationChain(
        llm=llm,
        memory=memory,
        prompt=chat_prompt,
        verbose=True
    ) | RunnableLambda(lambda x: {"response": x['response']}) | itemgetter("response")

    print()

    while True:
        msg = input("æˆ‘èªªï¼š")
        if not msg.strip():
            break

        if msg.strip() == "/è¨˜æ†¶":
            messages = memory.chat_memory.messages
            if messages:
                summary_text = summarize_existing_history(messages)
                memory.moving_summary_buffer = summary_text  # æ›´æ–°è¨˜æ†¶
                print(f"\nğŸ§  å·²æ›´æ–°æ‘˜è¦è¨˜æ†¶å…§å®¹å¦‚ä¸‹ï¼š\n{summary_text}\n")
            else:
                print("\nğŸ§  (ç›®å‰å°šç„¡æ‘˜è¦è¨˜æ†¶)\n")
            continue

        if msg.strip() == "/æ­·å²":
            print("\nğŸ—‚ï¸ SQLite å°è©±æ­·å²ï¼š")
            for message in memory.chat_memory.messages:
                role = "ğŸ§‘â€ğŸ¦± ä½¿ç”¨è€…" if message.type == "human" else "ğŸ¤– åŠ©ç†"
                print(f"{role}ï¼š{message.content}")
            print()
            continue

        response = chain.invoke({"input": msg})
        print(f'{sys_msg}ï¼š{response}\n')
